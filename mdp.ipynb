{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "modules = [\"maxent\", \"optimizer\", \"plot\", \"solver\", \"limiirl\", \"utils\"]\n",
    "def load_modules(modules): \n",
    "    for module in modules:\n",
    "        imported_module = importlib.import_module(module)\n",
    "        importlib.reload(imported_module)\n",
    "\n",
    "\n",
    "load_modules(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from datetime import datetime\n",
    "from maxent import irl, irl_causal, feature_expectation_from_trajectories\n",
    "import optimizer as O \n",
    "import solver as S                          # MDP solver (value-iteration)\n",
    "import plot as P\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from limiirl import limiirl\n",
    "from utils import *\n",
    "import importlib\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load patients, inputevents, and vital signs dataframe from MIMIC-IV \n",
    "\n",
    "patients_df = read_csv_to_dataframe(\"data/patients.csv\")\n",
    "# prescriptions_df = read_csv_to_dataframe(\"data/prescriptions.csv\")\n",
    "inputevents_df = read_csv_to_dataframe(\"data/inputevents.csv\")\n",
    "# procedureevents_df = read_csv_to_dataframe(\"data/procedureevents.csv\")\n",
    "# d_icd_diagnoses_df = read_csv_to_dataframe(\"data/d_icd_diagnoses.csv\")\n",
    "# triage_df = read_csv_to_dataframe(\"data/triage.csv\")\n",
    "vitalsign_df = read_csv_to_dataframe(\"data/vitalsign.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge vital signs and patients \n",
    "data_pv = pd.merge(patients_df, vitalsign_df, on='subject_id', how='inner')\n",
    "_ = fill_NANS(data_pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain unique heart rhythms \n",
    "unique_rhythms = data_pv['rhythm'].unique()\n",
    "n_rhythms = len(unique_rhythms)\n",
    "rhythms_mapping = {k:v for k,v in zip(unique_rhythms, range(n_rhythms))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features \n",
    "features = [\"gender\", \"anchor_age\", \"temperature\", \"heartrate\", \"resprate\", \"o2sat\", \"sbp\", \"dbp\", \"rhythm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = read_json(\"data/process/M.json\")\n",
    "M = np.array(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_S(M, n_clusters=100, random_state=42): \n",
    "    \"\"\"\n",
    "    M: matrix representation of data \n",
    "\n",
    "    returns a model that takes in a feature set D -> state s from 0 to K - 1\n",
    "    where K is the number of clusters \n",
    "    \"\"\"\n",
    "\n",
    "    # instantiate model and fit data\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(M)\n",
    "\n",
    "    # return model \n",
    "    return kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydenpersonnat/MI-IRL/.venv/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "state_model = discretize_S(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputevents_sample = inputevents_df.sample(n=6000000, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = len(inputevents_df['ordercategorydescription'].unique())\n",
    "actions = inputevents_df['ordercategorydescription'].unique() \n",
    "action_mapping = {k:v for k, v in zip(actions, range(n_actions))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_trajectories(p_events, p_vitals): \n",
    "    \"\"\"\n",
    "    p_events: events for each patient \n",
    "    p_vitals: vital readings for each patient \n",
    "    \"\"\"\n",
    "    trajs = {} \n",
    "    \n",
    "    for patient in p_events: \n",
    "        tau = trajs_from_patient(p_events[patient], p_vitals[patient])\n",
    "        # drop trajectories with length = 0\n",
    "        if (len(tau) > 1):\n",
    "            trajs[int(patient)] = trajs_from_patient(p_events[patient], p_vitals[patient])\n",
    "\n",
    "    return trajs \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_events = find_patient_events(inputevents_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_vitals = find_patient_vitals(data_pv, state_model, features, rhythms_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce invariant that vital and events data contains entries for both events and vitals   \n",
    "p_events, p_vitals = intersect_vitals_events(patient_events, patient_vitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = construct_trajectories(p_events, p_vitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(trajectories, \"data/process/trajs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories = read_json(\"data/process/trajs.json\")\n",
    "trajectories = { int(k) : v for k, v in zip(trajectories.keys(), trajectories.values()) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_traj(trajectories):\n",
    "    lst = []\n",
    "    for patient in trajectories:\n",
    "        traj = trajectories[patient]\n",
    "        row = []\n",
    "        n = len(traj)\n",
    "        for i in range(0, n-2, 2):\n",
    "            row.append((traj[i], traj[i+1], traj[i+2]))\n",
    "        \n",
    "        lst.append(row)\n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.9\n",
    "num_clusters = 100\n",
    "\n",
    "smoothing_value = 1\n",
    "\n",
    "p_transition = np.zeros((num_clusters, num_clusters, 5)) + smoothing_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = convert_traj(trajectories)\n",
    "\n",
    "for traj in T:\n",
    "\n",
    "  for tran in traj:\n",
    "\n",
    "    p_transition[tran[0], tran[2], tran[1]] +=1\n",
    "\n",
    "p_transition = p_transition/ p_transition.sum(axis = 1)[:, np.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminal_states = set()\n",
    "\n",
    "for patient in trajectories:\n",
    "    terminal_states.add(trajectories[patient][-1])\n",
    "\n",
    "terminal_states = list(terminal_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert states and actions to one-hot encoding\n",
    "state_encoder = OneHotEncoder(sparse=False, categories= [np.arange(num_clusters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydenpersonnat/MI-IRL/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# set up features: we use one feature vector per state (1 hot encoding for each cluster/state)\n",
    "features = state_encoder.fit_transform(np.arange(num_clusters).reshape(-1, 1))\n",
    "\n",
    "# choose our parameter initialization strategy:\n",
    "#   initialize parameters with constant\n",
    "init = O.Constant(1.0)\n",
    "\n",
    "# choose our optimization strategy:\n",
    "#   we select exponentiated stochastic gradient descent with linear learning-rate decay\n",
    "optim = O.ExpSga(lr=O.linear_decay(lr0=0.2))\n",
    "\n",
    "# actually do some inverse reinforcement learning\n",
    "# reward_maxent = maxent_irl(p_transition, features, terminal_states, trajectories, optim, init, eps= 1e-3)\n",
    "\n",
    "reward_maxent_causal, theta_causal = irl_causal(p_transition, features, terminal_states, T, optim, init, discount,\n",
    "               eps=1e-3, eps_svf=1e-4, eps_lap=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_policy(p_transition, reward, discount=0.9):\n",
    "    \"\"\" \n",
    "    p_transition: transition probabilities\n",
    "    reward: reward function \n",
    "    discount: discount factor \n",
    "\n",
    "    calculates V and Q function using value iteration \n",
    "    \n",
    "    returns softmax policy and policy \n",
    "    \"\"\" \n",
    "    V, Q = S.value_iteration(p_transition, reward, discount)\n",
    "    Q = Q.reshape((5, num_clusters))\n",
    "\n",
    "    # get softmax policy \n",
    "    soft_pi = (np.exp(Q)/ np.sum(np.exp(Q), axis = 0)).T\n",
    "    policy = np.argmax(Q, axis = 0).reshape(-1, )\n",
    "\n",
    "    return policy, soft_pi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "V, Q = S.value_iteration(p_transition, reward_maxent_causal, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Q.reshape((5, num_clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_pi_mce = (np.exp(Q)/ np.sum(np.exp(Q), axis = 0)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "soft_pi_mce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_mce = np.argmax(Q, axis = 0).reshape(-1, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy_mce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "n_patients = 100 \n",
    "K = 5\n",
    "gamma = 0.9\n",
    "\n",
    "\n",
    "random_patients = random.sample(list(trajectories.keys()), n_patients)\n",
    "\n",
    "trajectories_s = { patient: trajectories[patient] for patient in random_patients }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(tau, f, gamma): \n",
    "    \"\"\"\n",
    "    phi is a feature-based function that maps a trajectory `tau` -> R \n",
    "    \n",
    "    tau: trajectory of the form (s_1, a_1, s_2, a_2, ..., s_{|tau|})\n",
    "    f: feature function: S -> R\n",
    "    gamma: discount factor \n",
    "\n",
    "    phi(tau) = \\sum_{t = 1}^{|tau|}} gamma^{t - 1}phi(s_t) \n",
    "\n",
    "    define |tau| as number of (s_t, a_t, s_{t + 1}) tuples \n",
    "    \"\"\"\n",
    "\n",
    "    n = len(tau)\n",
    "    return np.sum([(gamma ** (i // 2)) * f[tau[i]]  for i in range(0, n, 2)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = feature_expectation_from_trajectories(features, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_trajectories(taus, gamma=0.9): \n",
    "    \"\"\" \n",
    "    taus: dictionary of the form { patient: tau } where tau is the patient's trajectory \n",
    "    during stay in ICU (from MIMIC dataset)\n",
    "\n",
    "    gamma: discount factor \n",
    "\n",
    "    returns X of shape N x d, where N is the number of trajectories and d represents number of features \n",
    "    \"\"\"\n",
    "    X = [] \n",
    "\n",
    "    for patient in taus: \n",
    "        traj = taus[patient] \n",
    "        phi_t = phi(traj, f, gamma)\n",
    "        X.append([phi_t])\n",
    "\n",
    "    return np.array(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_trajectories(trajectories_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_trajectories(X, n_experts=100): \n",
    "    \"\"\"\n",
    "    X: feature representation of trajectories\n",
    "    n_experts: assume trajectories come from `n_experts` experts \n",
    "\n",
    "    performs kmeans clustering on X and returns model \n",
    "    \"\"\"\n",
    "    kmeans = KMeans(n_clusters=n_experts, random_state=42).fit(X)\n",
    "\n",
    "    return kmeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaydenpersonnat/MI-IRL/.venv/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    }
   ],
   "source": [
    "cluster_model = cluster_trajectories(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {} # key: cluster, value: 2d list with the trajectories in that cluster \n",
    "\n",
    "for patient in trajectories: \n",
    "    traj = trajectories[patient]\n",
    "    phi_t = phi(traj, f, gamma)\n",
    "    cluster = cluster_model.predict([[phi_t]])[0]\n",
    "    if not cluster in data: \n",
    "        data[cluster] = [traj]\n",
    "    else: \n",
    "        data[cluster].append(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_traj(trajectories):\n",
    "    lst = []\n",
    "    for traj in trajectories:\n",
    "        row = []\n",
    "        n = len(traj)\n",
    "        for i in range(0, n-2, 2):\n",
    "            row.append((traj[i], traj[i+1], traj[i+2]))\n",
    "        \n",
    "        lst.append(row)\n",
    "    \n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(traj, theta, gamma=0.9): \n",
    "    # calculate reward \n",
    "    reward = features.dot(theta)\n",
    "\n",
    "    V, Q = S.value_iteration(p_transition, reward, gamma)\n",
    "    # calculate policy using softmax selection model \n",
    "\n",
    "    Q = Q.reshape((5, num_clusters))\n",
    "    soft_pi = (np.exp(Q)/ np.sum(np.exp(Q), axis = 0)).T\n",
    "     \n",
    "    prod = 1 \n",
    "    n = len(traj)\n",
    "    for i in range(0, n - 1, 2): \n",
    "        state = traj[i]\n",
    "        action = traj[i + 1]\n",
    "        prod *= soft_pi[state][action]\n",
    "\n",
    "    return prod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# N = 20 \n",
    "\n",
    "# x = np.arange(100)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# # Plotting each dataset on the same axes with a label for the legend\n",
    "# ax.plot(x, reward_maxent_causal, label='Single Intention')\n",
    "\n",
    "# for i in range(N): \n",
    "#     ax.plot(x, traj_models[i]['reward'], label=f'MI: Expert_{i}')\n",
    "   \n",
    "\n",
    "# # Adding the legend, which uses the labels specified in the plot commands\n",
    "# ax.legend()\n",
    "\n",
    "# # Setting the x-axis and y-axis labels\n",
    "# ax.set_xlabel('state')\n",
    "# ax.set_ylabel('reward')\n",
    "\n",
    "# # Setting the x-axis range from 0 to 99\n",
    "# ax.set_xlim([0, 99])\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10\n",
    "taus = list(trajectories_s.values())\n",
    "epsilon = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LiMIIRL: cluster 0\n",
      "LiMIIRL: cluster 1\n",
      "LiMIIRL: cluster 2\n",
      "LiMIIRL: cluster 3\n",
      "LiMIIRL: cluster 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jaydenpersonnat/MI-IRL/mdp.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jaydenpersonnat/MI-IRL/mdp.ipynb#Y102sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rho, theta, u \u001b[39m=\u001b[39m limiirl(X, taus, features, cluster_model, num_clusters, max_iter\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, K\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "File \u001b[0;32m~/MI-IRL/limiirl.py:162\u001b[0m, in \u001b[0;36mlimiirl\u001b[0;34m(X, taus, features, M, S, K, gamma, epsilon, max_iter)\u001b[0m\n\u001b[1;32m    158\u001b[0m p_transition \u001b[39m=\u001b[39m transition_model(T)\n\u001b[1;32m    160\u001b[0m terminal_states \u001b[39m=\u001b[39m calc_terminal_states(C[k])\n\u001b[0;32m--> 162\u001b[0m _, theta_k \u001b[39m=\u001b[39m irl_causal(p_transition, features, terminal_states, T, optim, init, gamma,\n\u001b[1;32m    163\u001b[0m             eps\u001b[39m=\u001b[39;49m\u001b[39m1e-3\u001b[39;49m, eps_svf\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m, eps_lap\u001b[39m=\u001b[39;49m\u001b[39m1e-4\u001b[39;49m)\n\u001b[1;32m    165\u001b[0m \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(S): \n\u001b[1;32m    166\u001b[0m     theta[k][s] \u001b[39m=\u001b[39m theta_k[s] \n",
      "File \u001b[0;32m~/MI-IRL/maxent.py:443\u001b[0m, in \u001b[0;36mirl_causal\u001b[0;34m(p_transition, features, terminal, trajectories, optim, init, discount, eps, eps_svf, eps_lap)\u001b[0m\n\u001b[1;32m    440\u001b[0m reward \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mdot(theta)\n\u001b[1;32m    442\u001b[0m \u001b[39m# compute the gradient\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m e_svf \u001b[39m=\u001b[39m compute_expected_causal_svf(p_transition, p_initial, terminal, reward, discount,\n\u001b[1;32m    444\u001b[0m                                     eps_lap, eps_svf)\n\u001b[1;32m    446\u001b[0m grad \u001b[39m=\u001b[39m e_features \u001b[39m-\u001b[39m features\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mdot(e_svf)\n\u001b[1;32m    448\u001b[0m \u001b[39m# perform optimization step and compute delta for convergence\u001b[39;00m\n",
      "File \u001b[0;32m~/MI-IRL/maxent.py:379\u001b[0m, in \u001b[0;36mcompute_expected_causal_svf\u001b[0;34m(p_transition, p_initial, terminal, reward, discount, eps_lap, eps_svf)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_expected_causal_svf\u001b[39m(p_transition, p_initial, terminal, reward, discount,\n\u001b[1;32m    345\u001b[0m                                 eps_lap\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m, eps_svf\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m):\n\u001b[1;32m    346\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \u001b[39m    Compute the expected state visitation frequency for maximum causal\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[39m    entropy IRL.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39m            threshold on all states in a single iteration.\u001b[39;00m\n\u001b[1;32m    378\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 379\u001b[0m     p_action \u001b[39m=\u001b[39m local_causal_action_probabilities(p_transition, terminal, reward, discount, eps_lap)\n\u001b[1;32m    380\u001b[0m     \u001b[39mreturn\u001b[39;00m expected_svf_from_policy(p_transition, p_initial, terminal, p_action, eps_svf)\n",
      "File \u001b[0;32m~/MI-IRL/maxent.py:329\u001b[0m, in \u001b[0;36mlocal_causal_action_probabilities\u001b[0;34m(p_transition, terminal, reward, discount, eps)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mwhile\u001b[39;00m delta \u001b[39m>\u001b[39m eps:\n\u001b[1;32m    327\u001b[0m     v_old \u001b[39m=\u001b[39m v\n\u001b[0;32m--> 329\u001b[0m     q \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([reward \u001b[39m+\u001b[39m discount \u001b[39m*\u001b[39m p[a]\u001b[39m.\u001b[39mdot(v_old) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_actions)])\u001b[39m.\u001b[39mT\n\u001b[1;32m    331\u001b[0m     v \u001b[39m=\u001b[39m reward_terminal\n\u001b[1;32m    332\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_actions):\n",
      "File \u001b[0;32m~/MI-IRL/maxent.py:329\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[39mwhile\u001b[39;00m delta \u001b[39m>\u001b[39m eps:\n\u001b[1;32m    327\u001b[0m     v_old \u001b[39m=\u001b[39m v\n\u001b[0;32m--> 329\u001b[0m     q \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([reward \u001b[39m+\u001b[39m discount \u001b[39m*\u001b[39m p[a]\u001b[39m.\u001b[39;49mdot(v_old) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_actions)])\u001b[39m.\u001b[39mT\n\u001b[1;32m    331\u001b[0m     v \u001b[39m=\u001b[39m reward_terminal\n\u001b[1;32m    332\u001b[0m     \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_actions):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rho, theta, u = limiirl(X, taus, features, cluster_model, num_clusters, max_iter=4, K=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_policy(pi, state, N=1000): \n",
    "    dist = pi[state]\n",
    "\n",
    "    actions = np.arange(len(dist))\n",
    "    sampled_actions = np.random.choice(actions, size=N, p=dist)\n",
    "\n",
    "    return sampled_actions  \n",
    "\n",
    "def policy_dist(pi_one, pi_two):\n",
    "    # soft policies - distribution of actions for each state \n",
    "\n",
    "    dist_vect = [] \n",
    "    for s in range(num_clusters): \n",
    "        sample_one = sample_policy(pi_one, s)\n",
    "        sample_two = sample_policy(pi_two, s)\n",
    "        dist_vect.append(wasserstein_distance(sample_one, sample_two)) \n",
    "\n",
    "    \n",
    "    return np.mean(dist_vect)\n",
    "\n",
    "def cmp_single(expert_policies, single_policy): \n",
    "    \"\"\"\n",
    "    expert_policies: arr of policies of shape K x |S| x |A|  \n",
    "    single_policy: policy of shape |S| x |A| \n",
    "    returns a vector of distances comparing kth expert with single intention model\n",
    "    \"\"\"\n",
    "\n",
    "    dist_vect = [] \n",
    "    for expert_pi in expert_policies: \n",
    "        dist = policy_dist(expert_pi, single_policy)\n",
    "        dist_vect.append(dist)\n",
    "\n",
    "    return np.array(dist_vect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
